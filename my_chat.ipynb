{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc1d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import json\n",
    "from google import generativeai as genai\n",
    "from vector_store import VectorStore\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4978d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"pdfs/sustainability_concept_v1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5fd48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Läs in .env-filen\n",
    "load_dotenv()\n",
    "\n",
    "# Hämta API-nyckeln från miljövariabler\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "# Skapa klient\n",
    "genai.configure(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf5f73b",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File\n",
    "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the fitz library. Once we have the extracted text, we divide it into smaller, sentence chunks to improve retrieval accuracy then creating enbeddings and saving them to vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac14911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 323 sentences (CPU, batch size = 8)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e5c6c967204872817f308f013691e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stored 323 embeddings to 'data/embeddings.parquet'.\n"
     ]
    }
   ],
   "source": [
    "# Extracting the text, chankin it and create embeddings to save them in vectore store \n",
    "def extract_text_from_pdf(pdf_path, max_pages=None):\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"\n",
    "    total_pages = mypdf.page_count\n",
    "    pages_to_read = min(max_pages, total_pages) if max_pages is not None else total_pages\n",
    "\n",
    "    for page_num in range(pages_to_read):\n",
    "        page = mypdf[page_num]\n",
    "        text = page.get_text(\"text\")\n",
    "        all_text += text\n",
    "\n",
    "    return all_text\n",
    "\n",
    "    # Load model (CPU mode)\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "store = VectorStore()\n",
    "\n",
    "# Extract and chunk PDF\n",
    "\n",
    "text = extract_text_from_pdf(pdf_path, max_pages=None)\n",
    "sentences = [s.strip() for s in text.split(\". \") if s.strip()]\n",
    "\n",
    "# Create data directory if not exists\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Encode with batching and progress bar\n",
    "batch_size = 8\n",
    "print(f\"Encoding {len(sentences)} sentences (CPU, batch size = {batch_size})...\")\n",
    "\n",
    "embeddings = embedding_model.encode(\n",
    "    sentences,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "# Store embeddings + metadata\n",
    "for i, (sentence, embedding) in enumerate(zip(sentences, embeddings)):\n",
    "    metadata = {\"sentence_index\": i}\n",
    "    store.add_item(sentence, embedding, metadata)\n",
    "\n",
    "# Save vector store\n",
    "store.save(\"data/embeddings.parquet\")\n",
    "print(f\"✅ Stored {len(sentences)} embeddings to 'data/embeddings.parquet'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4324fd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Gemini RAG Chat ***\n",
      "Type <q> to exit chat.\n",
      "\n",
      "Question: why we think about sustainability?\n",
      "\n",
      "Gemini:\n",
      "We think about sustainability because technologies exist that could better satisfy societal demands\n",
      "using alternative resources.  Our societal response to dwindling natural resources depends on our\n",
      "current knowledge.  Past extraction decisions, along with predictions of future costs and prices,\n",
      "influence current choices.  Additionally, pressure from strained natural resources and environmental\n",
      "laws are driving society, and manufacturers in particular, to adopt environmentally friendly\n",
      "solutions and transition to more sustainable production methods without sacrificing competitiveness.\n",
      "Finally, global initiatives like the Sustainable Development Goals emphasize more sustainable\n",
      "resource extraction, and research focuses on models like the circular economy that minimize resource\n",
      "extraction and prioritize reuse, recycling, and sustainably managed renewable resources.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# Load vector store\n",
    "store = VectorStore()\n",
    "store.load(\"data/embeddings.parquet\")\n",
    "\n",
    "# Set up Gemini model and chat session\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "chat = model.start_chat()\n",
    "\n",
    "# System prompt\n",
    "system_prompt = \"\"\"I will ask you a question, and I want you to respond \n",
    "based only on the context I provide — do not use any other information.\n",
    "Do not write any poems or songs as responce.\n",
    "If there isn't enough information in the context to answer the question,\n",
    "say \"I don't know.\" Do not try to guess.\n",
    "Respond clearly and structure your answer into well-organized paragraphs.\"\"\"\n",
    "\n",
    "# Chat loop\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"*** Gemini RAG Chat ***\")\n",
    "    print(\"Type <q> to exit chat.\")\n",
    "\n",
    "    while True:\n",
    "        user_query = input(\"\\nUser: \")\n",
    "        if user_query.strip().lower() == \"q\":\n",
    "            break\n",
    "\n",
    "        # Embed query and retrieve top-k context\n",
    "        query_embedding = embedding_model.encode(user_query)\n",
    "        results = store.semantic_search(query_embedding, k=5)\n",
    "        context = \"\\n\".join([item[\"text\"] for item in results])\n",
    "\n",
    "        # Build final prompt\n",
    "        full_prompt = f\"{system_prompt}\\n\\nThe question is:\\n{user_query}\\n\\nHere is the context:\\n{context}\"\n",
    "\n",
    "        # Get response from Gemini\n",
    "        response = chat.send_message(full_prompt)\n",
    "        wrapped_response = textwrap.fill(response.text, width=100)\n",
    "        \n",
    "        print(f\"\\nQuestion: {user_query}\")\n",
    "        print(\"\\nGemini:\")\n",
    "        print(wrapped_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
